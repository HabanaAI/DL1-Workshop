{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5783656d-2dc8-4e39-9cff-cb8bdd400617",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company.\n",
    "### Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "You may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0aebfb4e",
   "metadata": {},
   "source": [
    "# PyTorch BERT FineTuning Example on Habana Gaudi\n",
    "\n",
    "This Jupyter Notebook example demonstrates how to finetune BERT on Habana Gaudi device with PyTorch framework. The pretrained model will be downloaded from HuggingFace, and finetuned with SQuAD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425b3c11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd /root"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d125ba83",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d429c5c",
   "metadata": {},
   "source": [
    "Let's clone Habana `Model-References` repository to this image and add it to PYTHONPATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b9419f-1e41-4c28-978f-e6070afda1ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/habanaai/Model-References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591b7368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!export PYTHONPATH=/root/Model-References:$PYTHONPATH\n",
    "!export PYTHON=/usr/bin/python3.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470f2e57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd /root/Model-References/PyTorch/nlp/bert"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6baafb6-983e-4880-b623-fdf4be0061b4",
   "metadata": {},
   "source": [
    "Next, we need to install all the Python packages that BERT depends on.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93ea625",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 -m pip install --quiet -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90ce3280-9d63-4a51-bd3e-7bdfdd7a8fd1",
   "metadata": {},
   "source": [
    "Download the Vocab File.  This folder is using vocab.txt only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6742c339-831d-4b10-9271-e31d7de1bc62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-10_H-768_A-12.zip\n",
    "!unzip uncased_L-10_H-768_A-12.zip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e617c654-d4a8-45ef-86fb-727e1e50ca3d",
   "metadata": {},
   "source": [
    "Download the checkpoints from the Habana Vault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01940167-bf42-4f24-8001-5eacfd0d3b82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://vault.habana.ai/artifactory/pretrained-models/checkpoints/1.9.0/PT/BertL-PT/BertL-PT-PyTorch-1.13.1-1.9.0-580-32n-checkpoint.tar.gz\n",
    "!tar -xvzf BertL-PT-PyTorch-1.13.1-1.9.0-580-32n-checkpoint.tar.gz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b0bd4cb-55a2-4c91-a155-479e2a850465",
   "metadata": {
    "tags": []
   },
   "source": [
    "Download and pre-process Squad Dataset(V1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd22eea8-f4af-4d45-a3f3-47946545bb99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sh ./data/squad/squad_download.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce02235-742d-489b-8271-3e4e6221ba5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "158cda34",
   "metadata": {},
   "source": [
    "## Training on 1 HPU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fca8918c",
   "metadata": {},
   "source": [
    "After all the dependant Python packages are installed, let's launch BERT base finetuning with SQuAD dataset on a single HPU:\n",
    "\n",
    "We see that the original BERT model is now Fine Tuned with the SQuAD dataset \n",
    "\n",
    "```\n",
    "      $PYTHON run_squad.py --do_train --bert_model=bert-large-uncased --config_file=./bert_config.json \\\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a36bd03-9c07-43ed-a28f-e787e18f647e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd /root/Model-References/PyTorch/nlp/bert\n",
    "%mkdir /tmp/log_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76cd25f-d590-4143-8a14-72707f5ab49b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 run_squad.py --do_train --bert_model=bert-large-uncased --config_file=./bert_config.json \\\n",
    " --use_habana --use_fused_adam --do_lower_case --output_dir=/tmp/results/checkpoints \\\n",
    " --json-summary=/tmp/log_directory/dllogger.json \\\n",
    " --train_batch_size=12 --predict_batch_size=8 --seed=1 --max_seq_length=384 \\\n",
    " --doc_stride=128 --max_steps=-1   --learning_rate=3e-5 --num_train_epochs=2  \\\n",
    " --vocab_file=vocab.txt \\\n",
    " --init_checkpoint=ckpt_8601.pt  \\\n",
    " --train_file=./v1.1/train-v1.1.json \\\n",
    " --skip_cache --do_predict  \\\n",
    " --predict_file=./v1.1/dev-v1.1.json \\\n",
    " --do_eval --eval_script=./v1.1/evaluate-v1.1.py --log_freq 20 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8913fa76",
   "metadata": {},
   "source": [
    "**From the logs above, we can see the finetuning throughput for BERT base on 1 HPU is over 100 samples/second.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
