{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5783656d-2dc8-4e39-9cff-cb8bdd400617",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Habana Labs, Ltd. an Intel Company.\n",
    "### Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "You may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aebfb4e",
   "metadata": {},
   "source": [
    "# PyTorch BERT FineTuning Example on Habana Gaudi\n",
    "\n",
    "This Jupyter Notebook example demonstrates how to finetune BERT on Habana Gaudi device with PyTorch framework. The pretrained model will be downloaded from HuggingFace, and finetuned with SQuAD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "425b3c11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root\n"
     ]
    }
   ],
   "source": [
    "%cd /root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125ba83",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d429c5c",
   "metadata": {},
   "source": [
    "Let's clone Habana `Model-References` repository to this image and add it to PYTHONPATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6b9419f-1e41-4c28-978f-e6070afda1ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Model-References'...\n",
      "remote: Enumerating objects: 15297, done.\u001b[K\n",
      "remote: Counting objects: 100% (15296/15296), done.\u001b[K\n",
      "remote: Compressing objects: 100% (6684/6684), done.\u001b[K\n",
      "remote: Total 15297 (delta 8265), reused 15161 (delta 8161), pack-reused 1\u001b[K\n",
      "Receiving objects: 100% (15297/15297), 101.61 MiB | 54.47 MiB/s, done.\n",
      "Resolving deltas: 100% (8265/8265), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/habanaai/Model-References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591b7368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!export PYTHONPATH=/root/Model-References:$PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "470f2e57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Model-References/PyTorch/nlp/finetuning/huggingface/bert\n"
     ]
    }
   ],
   "source": [
    "%cd /root/Model-References/PyTorch/nlp/finetuning/huggingface/bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307cd4c6",
   "metadata": {},
   "source": [
    "Next, we need to install all the Python packages that BERT depends on.  Including HuggingFace Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e93ea625",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install dill>=0.3.6 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "872b0c30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.8/dist-packages (from -r ./transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: datasets>=1.8.0 in /usr/local/lib/python3.8/dist-packages (from -r ./transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2.10.1)\n",
      "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from -r ./transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (1.13.1a0+git0b11ee5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from accelerate->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (5.9.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from accelerate->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from accelerate->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (1.22.3)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets>=1.8.0->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets>=1.8.0->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=1.8.0->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (0.12.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=1.8.0->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2023.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=1.8.0->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2.28.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets>=1.8.0->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (3.2.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=1.8.0->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (4.64.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=1.8.0->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (0.3.6)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets>=1.8.0->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (0.18.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets>=1.8.0->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (1.4.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=1.8.0->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (11.0.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.3.0->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (4.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=1.8.0->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=1.8.0->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=1.8.0->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=1.8.0->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=1.8.0->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=1.8.0->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=1.8.0->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (3.0.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets>=1.8.0->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (3.9.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets>=1.8.0->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets>=1.8.0->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.8.0->-r ./transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ./transformers/examples/pytorch/question-answering/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1834b15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./transformers\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.20.1) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.20.1) (0.12.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.20.1) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.20.1) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.20.1) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.20.1) (2020.10.28)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.20.1) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.20.1) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.20.1) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.20.1) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.20.1) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.20.1) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.20.1) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.20.1) (3.4)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.20.1-py3-none-any.whl size=4248799 sha256=e8830417d7d3cd1893b5d574d13bce05863da662f94e82ef3a9aed301611a5b0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0ythz61l/wheels/26/3d/59/fdd991f9963e428015334930a898b7b10f9c4405d8fff2f52e\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.20.1\n",
      "    Uninstalling transformers-4.20.1:\n",
      "      Successfully uninstalled transformers-4.20.1\n",
      "Successfully installed transformers-4.20.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158cda34",
   "metadata": {},
   "source": [
    "## Training on 1 HPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca8918c",
   "metadata": {},
   "source": [
    "After all the dependant Python packages are installed, let's launch BERT base finetuning with SQuAD dataset on a single HPU in BF16 data type:\n",
    "\n",
    "We see that the original BERT model is now Fine Tuned with the SQuAD dataset \n",
    "\n",
    "``` \n",
    "python3 transformers/examples/pytorch/question-answering/run_qa.py --hmp --hmp_bf16=./ops_bf16_bert.txt --hmp_fp32=./ops_fp32_bert.txt --doc_stride=128 --use_lazy_mode --per_device_train_batch_size=12 --per_device_eval_batch_size=8 --dataset_name=squad --use_fused_adam --use_fused_clip_norm --use_hpu --max_seq_length=384 --learning_rate=3e-05 --num_train_epochs=1 --max_steps 1000 --output_dir=./output --logging_steps=40 --overwrite_output_dir --do_train --save_steps=8000 --model_name_or_path=bert-base-uncased\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59e1842b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/habana_frameworks/torch/utils/distributed_utils.py:6: UserWarning: habana_frameworks.torch.utils.distributed_utils.initialize_distributed_hpu is deprecated. Please use habana_frameworks.torch.distributed.hccl.initialize_distributed_hpu\n",
      "  warnings.warn(\"habana_frameworks.torch.utils.distributed_utils.initialize_distributed_hpu is deprecated. \"\n",
      "03/07/2023 21:06:51 - WARNING - __main__ - Process rank: -1, device: hpu, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "03/07/2023 21:06:51 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hmp=True,\n",
      "hmp_bf16=./ops_bf16_bert.txt,\n",
      "hmp_fp32=./ops_fp32_bert.txt,\n",
      "hmp_opt_level=O1,\n",
      "hmp_verbose=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_device_mem_alloc=False,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./output/runs/Mar07_21-06-51_habana-webinar-109-1,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=20,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=1000,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=./output,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=12,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./output,\n",
      "save_on_each_node=False,\n",
      "save_steps=8000,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_fused_adam=True,\n",
      "use_fused_clip_norm=True,\n",
      "use_hpu=True,\n",
      "use_ipex=False,\n",
      "use_lazy_mode=True,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "03/07/2023 21:06:52 - INFO - datasets.builder - No config specified, defaulting to the single config: squad/plain_text\n",
      "03/07/2023 21:06:52 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\n",
      "03/07/2023 21:06:52 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "03/07/2023 21:06:52 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\n",
      "03/07/2023 21:06:52 - WARNING - datasets.builder - Found cached dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "03/07/2023 21:06:52 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 522.82it/s]\n",
      "[INFO|configuration_utils.py:659] 2023-03-07 21:06:52,042 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:708] 2023-03-07 21:06:52,050 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:659] 2023-03-07 21:06:52,082 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:708] 2023-03-07 21:06:52,082 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1781] 2023-03-07 21:06:52,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|tokenization_utils_base.py:1781] 2023-03-07 21:06:52,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|tokenization_utils_base.py:1781] 2023-03-07 21:06:52,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1781] 2023-03-07 21:06:52,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1781] 2023-03-07 21:06:52,188 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "[INFO|configuration_utils.py:659] 2023-03-07 21:06:52,203 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:708] 2023-03-07 21:06:52,204 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2115] 2023-03-07 21:06:52,243 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "[WARNING|modeling_utils.py:2481] 2023-03-07 21:06:53,540 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2493] 2023-03-07 21:06:53,540 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "03/07/2023 21:06:53 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-52f1c67e9d15cce9.arrow\n",
      "transformers/examples/pytorch/question-answering/run_qa.py:596: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"squad_v2\" if data_args.version_2_with_negative else \"squad\")\n",
      "[INFO|trainer.py:438] 2023-03-07 21:06:53,765 >> Enabled lazy mode\n",
      "=============================HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 1\n",
      " PT_HPU_LAZY_EAGER_OPTIM_CACHE = 1\n",
      " PT_HPU_ENABLE_COMPILE_THREAD = 0\n",
      " PT_HPU_ENABLE_EXECUTION_THREAD = 1\n",
      " PT_HPU_ENABLE_LAZY_EAGER_EXECUTION_THREAD = 1\n",
      " PT_ENABLE_INTER_HOST_CACHING = 0\n",
      " PT_ENABLE_INFERENCE_MODE = 1\n",
      " PT_ENABLE_HABANA_CACHING = 1\n",
      " PT_HPU_MAX_RECIPE_SUBMISSION_LIMIT = 0\n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE_SS = 10\n",
      " PT_HPU_ENABLE_STAGE_SUBMISSION = 1\n",
      " PT_HPU_STAGE_SUBMISSION_MODE = 2\n",
      " PT_HPU_PGM_ENABLE_CACHE = 1\n",
      " PT_HPU_ENABLE_LAZY_COLLECTIVES = 0\n",
      " PT_HCCL_SLICE_SIZE_MB = 16\n",
      " PT_HCCL_MEMORY_ALLOWANCE_MB = 0\n",
      " PT_HPU_INITIAL_WORKSPACE_SIZE = 0\n",
      " PT_HABANA_POOL_SIZE = 24\n",
      " PT_HPU_POOL_STRATEGY = 5\n",
      " PT_HPU_POOL_LOG_FRAGMENTATION_INFO = 0\n",
      " PT_ENABLE_MEMORY_DEFRAGMENTATION = 1\n",
      " PT_ENABLE_DEFRAGMENTATION_INFO = 0\n",
      " PT_HPU_ENABLE_SYNAPSE_LAYOUT_HANDLING = 1\n",
      " PT_HPU_ENABLE_SYNAPSE_OUTPUT_PERMUTE = 1\n",
      " PT_HPU_ENABLE_VALID_DATA_RANGE_CHECK = 1\n",
      " PT_HPU_FORCE_USE_DEFAULT_STREAM = 0\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      " PT_HPU_DYNAMIC_MIN_POLICY_ORDER = 4,5,3,1\n",
      " PT_HPU_DYNAMIC_MAX_POLICY_ORDER = 2,4,5,3,1\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      "=============================SYSTEM CONFIGURATION ========================================= \n",
      "Num CPU Cores = 96\n",
      "CPU RAM = 784300912 KB \n",
      "============================================================================================ \n",
      "[INFO|trainer.py:495] 2023-03-07 21:06:56,353 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "hmp:verbose_mode  False\n",
      "hmp:opt_level O1\n",
      "[INFO|trainer.py:1553] 2023-03-07 21:06:56,434 >> ***** Running training *****\n",
      "[INFO|trainer.py:1554] 2023-03-07 21:06:56,434 >>   Num examples = 88524\n",
      "[INFO|trainer.py:1555] 2023-03-07 21:06:56,434 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1556] 2023-03-07 21:06:56,434 >>   Instantaneous batch size per device = 12\n",
      "[INFO|trainer.py:1557] 2023-03-07 21:06:56,434 >>   Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "[INFO|trainer.py:1558] 2023-03-07 21:06:56,434 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1559] 2023-03-07 21:06:56,434 >>   Total optimization steps = 1000\n",
      "{'loss': 5.3, 'learning_rate': 2.94e-05, 'epoch': 0.0}                          \n",
      "{'loss': 4.425, 'learning_rate': 2.88e-05, 'epoch': 0.01}                       \n",
      "{'loss': 4.1, 'learning_rate': 2.8199999999999998e-05, 'epoch': 0.01}           \n",
      "{'loss': 3.625, 'learning_rate': 2.7600000000000003e-05, 'epoch': 0.01}         \n",
      "{'loss': 3.05, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.01}          \n",
      "{'loss': 3.025, 'learning_rate': 2.64e-05, 'epoch': 0.02}                       \n",
      "{'loss': 2.575, 'learning_rate': 2.58e-05, 'epoch': 0.02}                       \n",
      "{'loss': 2.6625, 'learning_rate': 2.52e-05, 'epoch': 0.02}                      \n",
      "{'loss': 2.4625, 'learning_rate': 2.4599999999999998e-05, 'epoch': 0.02}        \n",
      "{'loss': 2.475, 'learning_rate': 2.4e-05, 'epoch': 0.03}                        \n",
      "{'loss': 2.3375, 'learning_rate': 2.3400000000000003e-05, 'epoch': 0.03}        \n",
      "{'loss': 2.2375, 'learning_rate': 2.2800000000000002e-05, 'epoch': 0.03}        \n",
      "{'loss': 2.3375, 'learning_rate': 2.22e-05, 'epoch': 0.04}                      \n",
      "{'loss': 2.2125, 'learning_rate': 2.16e-05, 'epoch': 0.04}                      \n",
      "{'loss': 2.05, 'learning_rate': 2.1e-05, 'epoch': 0.04}                         \n",
      "{'loss': 1.925, 'learning_rate': 2.04e-05, 'epoch': 0.04}                       \n",
      "{'loss': 1.95, 'learning_rate': 1.98e-05, 'epoch': 0.05}                        \n",
      "{'loss': 1.9, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.05}           \n",
      "{'loss': 1.7625, 'learning_rate': 1.86e-05, 'epoch': 0.05}                      \n",
      "{'loss': 1.85, 'learning_rate': 1.8e-05, 'epoch': 0.05}                         \n",
      "{'loss': 1.8375, 'learning_rate': 1.74e-05, 'epoch': 0.06}                      \n",
      "{'loss': 1.5063, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.06}        \n",
      "{'loss': 1.95, 'learning_rate': 1.62e-05, 'epoch': 0.06}                        \n",
      "{'loss': 1.5938, 'learning_rate': 1.56e-05, 'epoch': 0.07}                      \n",
      "{'loss': 1.775, 'learning_rate': 1.5e-05, 'epoch': 0.07}                        \n",
      "{'loss': 1.6625, 'learning_rate': 1.44e-05, 'epoch': 0.07}                      \n",
      "{'loss': 1.4812, 'learning_rate': 1.3800000000000002e-05, 'epoch': 0.07}        \n",
      "{'loss': 1.6625, 'learning_rate': 1.32e-05, 'epoch': 0.08}                      \n",
      "{'loss': 1.6375, 'learning_rate': 1.26e-05, 'epoch': 0.08}                      \n",
      "{'loss': 1.6375, 'learning_rate': 1.2e-05, 'epoch': 0.08}                       \n",
      "{'loss': 1.8, 'learning_rate': 1.1400000000000001e-05, 'epoch': 0.08}           \n",
      "{'loss': 1.475, 'learning_rate': 1.08e-05, 'epoch': 0.09}                       \n",
      "{'loss': 1.4563, 'learning_rate': 1.02e-05, 'epoch': 0.09}                      \n",
      "{'loss': 1.7, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.09}            \n",
      "{'loss': 1.6375, 'learning_rate': 9e-06, 'epoch': 0.09}                         \n",
      "{'loss': 1.7, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.1}             \n",
      "{'loss': 1.6125, 'learning_rate': 7.8e-06, 'epoch': 0.1}                        \n",
      "{'loss': 1.3375, 'learning_rate': 7.2e-06, 'epoch': 0.1}                        \n",
      "{'loss': 1.4375, 'learning_rate': 6.6e-06, 'epoch': 0.11}                       \n",
      "{'loss': 1.6625, 'learning_rate': 6e-06, 'epoch': 0.11}                         \n",
      "{'loss': 1.475, 'learning_rate': 5.4e-06, 'epoch': 0.11}                        \n",
      "{'loss': 1.5625, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.11}         \n",
      "{'loss': 1.45, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.12}          \n",
      "{'loss': 1.3438, 'learning_rate': 3.6e-06, 'epoch': 0.12}                       \n",
      "{'loss': 1.3938, 'learning_rate': 3e-06, 'epoch': 0.12}                         \n",
      "{'loss': 1.7625, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.12}        \n",
      "{'loss': 1.3438, 'learning_rate': 1.8e-06, 'epoch': 0.13}                       \n",
      "{'loss': 1.625, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.13}         \n",
      "{'loss': 1.4625, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.13}         \n",
      "{'loss': 1.5562, 'learning_rate': 0.0, 'epoch': 0.14}                           \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [01:52<00:00, 17.38it/s][INFO|trainer.py:1816] 2023-03-07 21:08:48,994 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 112.5637, 'train_samples_per_second': 106.606, 'train_steps_per_second': 8.884, 'train_loss': 2.036, 'epoch': 0.14}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [01:52<00:00,  8.88it/s]\n",
      "[INFO|trainer.py:2558] 2023-03-07 21:08:48,999 >> Saving model checkpoint to ./output\n",
      "[INFO|configuration_utils.py:446] 2023-03-07 21:08:48,999 >> Configuration saved in ./output/config.json\n",
      "[INFO|modeling_utils.py:1668] 2023-03-07 21:08:58,368 >> Model weights saved in ./output/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2123] 2023-03-07 21:08:58,369 >> tokenizer config file saved in ./output/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2130] 2023-03-07 21:08:58,369 >> Special tokens file saved in ./output/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       0.14\n",
      "  train_loss               =      2.036\n",
      "  train_runtime            = 0:01:52.56\n",
      "  train_samples            =      88524\n",
      "  train_samples_per_second =    106.606\n",
      "  train_steps_per_second   =      8.884\n",
      "[INFO|modelcard.py:460] 2023-03-07 21:08:58,433 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad', 'type': 'squad', 'args': 'plain_text'}}\n"
     ]
    }
   ],
   "source": [
    "!python3 transformers/examples/pytorch/question-answering/run_qa.py --hmp --hmp_bf16=./ops_bf16_bert.txt --hmp_fp32=./ops_fp32_bert.txt --doc_stride=128 --use_lazy_mode --per_device_train_batch_size=12 --per_device_eval_batch_size=8 --dataset_name=squad --use_fused_adam --use_fused_clip_norm --use_hpu --max_seq_length=384 --learning_rate=3e-05 --num_train_epochs=1 --max_steps 1000 --output_dir=./output --logging_steps=20 --overwrite_output_dir --do_train --save_steps=8000 --model_name_or_path=bert-base-uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8913fa76",
   "metadata": {},
   "source": [
    "**From the logs above, we can see the finetuning throughput for BERT base on 1 HPU is over 100 samples/second.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
