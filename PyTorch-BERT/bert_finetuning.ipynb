{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aebfb4e",
   "metadata": {},
   "source": [
    "# PyTorch BERT FineTuning Example on Habana Gaudi\n",
    "\n",
    "This Jupyter Notebook example demonstrates how to finetune BERT on Habana Gaudi device with PyTorch framework. The pretrained model will be downloaded from HuggingFace, and finetuned with SQuAD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b333a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425b3c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125ba83",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d429c5c",
   "metadata": {},
   "source": [
    "Since we have already cloned Habana `Model-References` repository to this DLAMI, now let's add it to PYTHONPATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591b7368",
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env PYTHONPATH=/home/ubuntu/Model-References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470f2e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ubuntu/Model-References/PyTorch/nlp/finetuning/huggingface/bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307cd4c6",
   "metadata": {},
   "source": [
    "Next, we need to install all the Python packages that BERT depends on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93ea625",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dill>=0.3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b0c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ./transformers/examples/pytorch/question-answering/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1834b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158cda34",
   "metadata": {},
   "source": [
    "## Training on 1 HPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca8918c",
   "metadata": {},
   "source": [
    "After all the dependant Python packages are installed, let's launch BERT base finetuning with SQuAD dataset on a single HPU in BF16 data type:\n",
    "\n",
    "``` \n",
    "python3 transformers/examples/pytorch/question-answering/run_qa.py --hmp --hmp_bf16=./ops_bf16_bert.txt --hmp_fp32=./ops_fp32_bert.txt --doc_stride=128 --use_lazy_mode --per_device_train_batch_size=12 --per_device_eval_batch_size=8 --dataset_name=squad --use_fused_adam --use_fused_clip_norm --use_hpu --max_seq_length=384 --learning_rate=3e-05 --num_train_epochs=1 --max_steps 1000 --output_dir=./output --logging_steps=40 --overwrite_output_dir --do_train --save_steps=8000 --model_name_or_path=bert-base-uncased\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e1842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 transformers/examples/pytorch/question-answering/run_qa.py --hmp --hmp_bf16=./ops_bf16_bert.txt --hmp_fp32=./ops_fp32_bert.txt --doc_stride=128 --use_lazy_mode --per_device_train_batch_size=12 --per_device_eval_batch_size=8 --dataset_name=squad --use_fused_adam --use_fused_clip_norm --use_hpu --max_seq_length=384 --learning_rate=3e-05 --num_train_epochs=1 --max_steps 1000 --output_dir=./output --logging_steps=20 --overwrite_output_dir --do_train --save_steps=8000 --model_name_or_path=bert-base-uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8913fa76",
   "metadata": {},
   "source": [
    "From the logs above, we can see the finetuning throughput for BERT base on 1 HPU is around 88 samples/second."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
