{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet152 PyTorch Training on Gaudi\n",
    "\n",
    "In this notebook we will demonstrate how you can train the resnet152 image classifier using Pytorch. We will first demonstrate training on a single HPU, then on 8 HPUs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and prepare the CIFAR10 dataset\n",
    "\n",
    "\n",
    "The CIFAR10 dataset contains 60,000 color images in 10 classes, with 6,000 images in each class. The dataset is divided into 50,000 training images and 10,000 testing images. The classes are mutually exclusive and there is no overlap between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for running the training\n",
    "\n",
    "Set the python path environment variable and cd into appropriate directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env PYTHONPATH=/home/ubuntu/Model-References/PyTorch/computer_vision/classification/torchvision:/root/examples/models:/usr/lib/habanalabs/:/root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ubuntu/Model-References/PyTorch/computer_vision/classification/torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps required for migrating the model to Habana HPU\n",
    "Here are the lines of code we need to target the Habana device:\n",
    "\n",
    "1. Import the Habana Torch Library like this: \n",
    "```\n",
    "import habana_frameworks.torch.core as htcore\n",
    "```\n",
    "\n",
    "2. Target the Gaudi HPU device:\n",
    "```\n",
    "device = torch.device('hpu')\n",
    "```\n",
    "\n",
    "3. Add mark_step():\n",
    "```\n",
    "htcore.mark_step()\n",
    "```\n",
    "In Lazy mode, mark_step() must be added in all training scripts right after loss.backward() and optimizer.step(). The Habana bridge internally accumulates these ops in a graph. The execution of the ops in the accumulated graph is triggered only when a tensor value is required by the user. This allows the bridge to construct a SynapseAI graph with multiple ops, which provides the graph compiler the opportunity to optimize the device execution for these ops. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import libraries necessary for pytorch training and define the argument parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "import random\n",
    "import utils\n",
    "\n",
    "# Key changes for targetting the Habana Device\n",
    "import habana_frameworks.torch.core as htcore\n",
    "device = torch.device('hpu')\n",
    "\n",
    "def get_resnet152_argparser():\n",
    "    import argparse\n",
    "    import sys\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Classification Training')\n",
    "    parser.add_argument('--dl-time-exclude', default='True', type=lambda x: x.lower() == 'true', help='Set to False to include data load time')\n",
    "    parser.add_argument('-b', '--batch-size', default=128, type=int)\n",
    "    parser.add_argument('--device', default='hpu', help='device')\n",
    "    parser.add_argument('--epochs', default=90, type=int, metavar='N',\n",
    "                        help='number of total epochs to run')\n",
    "    parser.add_argument('-j', '--workers', default=10, type=int, metavar='N',\n",
    "                        help='number of data loading workers (default: 10)')\n",
    "    parser.add_argument('--process-per-node', default=8, type=int, metavar='N',\n",
    "                        help='Number of process per node')\n",
    "    parser.add_argument('--hls_type', default='HLS1', help='Node type')\n",
    "    parser.add_argument('--lr', default=0.1, type=float, help='initial learning rate')\n",
    "    parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                        help='momentum')\n",
    "    parser.add_argument('--wd', '--weight-decay', default=1e-4, type=float,\n",
    "                        metavar='W', help='weight decay (default: 1e-4)',\n",
    "                        dest='weight_decay')\n",
    "\n",
    "    parser.add_argument('--print-freq', default=1, type=int, help='print frequency')\n",
    "    parser.add_argument('--output-dir', default='.', help='path where to save')\n",
    "\n",
    "    parser.add_argument('--channels-last', default='True', type=lambda x: x.lower() == 'true',\n",
    "                        help='Whether input is in channels last format.'\n",
    "                        'Any value other than True(case insensitive) disables channels-last')\n",
    "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
    "    parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "    parser.add_argument('--seed', type=int, default=123, help='random seed')\n",
    "    parser.add_argument('--world-size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--num-train-steps', type=int, default=sys.maxsize, metavar='T',\n",
    "                        help='number of steps a.k.a iterations to run in training phase')\n",
    "    parser.add_argument('--num-eval-steps', type=int, default=sys.maxsize, metavar='E',\n",
    "                        help='number of steps a.k.a iterations to run in evaluation phase')\n",
    "    parser.add_argument('--save-checkpoint', action=\"store_true\",\n",
    "                        help='Whether or not to save model/checkpont; True: to save, False to avoid saving')\n",
    "    parser.add_argument('--run-lazy-mode', action='store_true',\n",
    "                        help='run model in lazy execution mode')\n",
    "    parser.add_argument('--deterministic', action=\"store_true\",\n",
    "                        help='Whether or not to make data loading deterministic;This does not make execution deterministic')\n",
    "    parser.add_argument('--hmp', dest='is_hmp', action='store_true', help='enable hmp mode')\n",
    "    parser.add_argument('--hmp-bf16', default='', help='path to bf16 ops list in hmp O1 mode')\n",
    "    parser.add_argument('--hmp-fp32', default='', help='path to fp32 ops list in hmp O1 mode')\n",
    "    parser.add_argument('--hmp-opt-level', default='O1', help='choose optimization level for hmp')\n",
    "    parser.add_argument('--hmp-verbose', action='store_true', help='enable verbose mode for hmp')\n",
    "\n",
    "    return parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main training function \n",
    "Uncomment the mark_step code in the appropriate places (after backward loss computation and after optimizer step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, criterion, optimizer, data_loader, device, epoch, print_freq):\n",
    "    model.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \",device=device)\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value}'))\n",
    "    metric_logger.add_meter('img/s', utils.SmoothedValue(window_size=10, fmt='{value}'))\n",
    "\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    step_count = 0\n",
    "    last_print_time= time.time()\n",
    "\n",
    "    for image, target in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        image, target = image.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "\n",
    "        dl_ex_start_time=time.time()\n",
    "\n",
    "        if args.channels_last:\n",
    "            image = image.contiguous(memory_format=torch.channels_last)\n",
    "\n",
    "        output = model(image)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        loss.backward()\n",
    "        # Trigger graph execution\n",
    "        #htcore.mark_step()\n",
    "\n",
    "        optimizer.step()\n",
    "        # Trigger graph execution\n",
    "        #htcore.mark_step()\n",
    "\n",
    "        if step_count % print_freq == 0:\n",
    "            output_cpu = output.detach().to('cpu')\n",
    "            acc1, acc5 = utils.accuracy(output_cpu, target, topk=(1, 5))\n",
    "            batch_size = image.shape[0]\n",
    "            metric_logger.update(loss=loss.item(), lr=optimizer.param_groups[0][\"lr\"])\n",
    "            metric_logger.meters['acc1'].update(acc1.item(), n=batch_size*print_freq)\n",
    "            metric_logger.meters['acc5'].update(acc5.item(), n=batch_size*print_freq)\n",
    "            current_time = time.time()\n",
    "            last_print_time = dl_ex_start_time if args.dl_time_exclude else last_print_time\n",
    "            metric_logger.meters['img/s'].update(batch_size*print_freq / (current_time - last_print_time))\n",
    "            last_print_time = time.time()\n",
    "\n",
    "        step_count = step_count + 1\n",
    "        if step_count >= args.num_train_steps:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup necessary environment variables and command line arguments for single HPU resnet152 training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MAX_WAIT_ATTEMPTS\"] = \"50\"\n",
    "os.environ['HCL_CPU_AFFINITY'] = '1'\n",
    "os.environ['PT_HPU_ENABLE_SYNC_OUTPUT_HOST'] = 'false'\n",
    "parser = get_resnet152_argparser()\n",
    "   \n",
    "args = parser.parse_args([\"--batch-size\", \"256\", \"--epochs\", \"20\", \"--workers\", \"8\",\n",
    "\"--dl-time-exclude\", \"False\", \"--print-freq\", \"20\", \"--channels-last\", \"False\", \"--seed\", \"123\", \n",
    "\"--run-lazy-mode\", \"--hmp\",  \"--hmp-bf16\", \"/home/ubuntu/Model-References/PyTorch/computer_vision/classification/torchvision/ops_bf16_Resnet.txt\",\n",
    "\"--hmp-fp32\", \"/home/ubuntu/Model-References/PyTorch/computer_vision/classification/torchvision/ops_fp32_Resnet.txt\",\n",
    "\"--deterministic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMP Usage and why it’s important\n",
    "Habana Mixed Precision (HMP) package is a tool that allows you to run mixed precision training on HPU without extensive modifications to existing FP32 model scripts. You can easily add mixed precision training support to the model script by adding the following lines anywhere in the script before the start of the training loop:\n",
    "```\n",
    "from habana_frameworks.torch.hpex import hmp \n",
    "hmp.convert()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main training code block for single node training. Using CIFAR data to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.is_hmp:\n",
    "    from habana_frameworks.torch.hpex import hmp\n",
    "    hmp.convert(opt_level=args.hmp_opt_level, bf16_file_path=args.hmp_bf16,\n",
    "                fp32_file_path=args.hmp_fp32, isVerbose=args.hmp_verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if args.deterministic:\n",
    "    seed = args.seed\n",
    "    random.seed(seed)\n",
    "else:\n",
    "    seed = None\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "if args.workers > 0:\n",
    "    # patch torch cuda functions that are being unconditionally invoked\n",
    "    # in the multiprocessing data loader\n",
    "    torch.cuda.current_device = lambda: None\n",
    "    torch.cuda.set_device = lambda x: None\n",
    "\n",
    "print(\"Creating model\")\n",
    "model = torchvision.models.__dict__['resnet152'](pretrained=False)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if args.run_lazy_mode:\n",
    "    from habana_frameworks.torch.hpex.optimizers import FusedSGD\n",
    "    sgd_optimizer = FusedSGD\n",
    "else:\n",
    "    sgd_optimizer = torch.optim.SGD\n",
    "optimizer = sgd_optimizer(\n",
    "    model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "model_for_train = model\n",
    "\n",
    "\n",
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    train_one_epoch(model_for_train, criterion, optimizer, trainloader,\n",
    "            device, epoch, print_freq=args.print_freq)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Training\n",
    "\n",
    "**Restart the kernel before running the next section of the notebook**\n",
    "\n",
    "We will use the Model-References repository command line to demo distributed training on 8 HPUs. \n",
    "\n",
    "Distributed training differs in the following ways.\n",
    "\n",
    "1. [Initialization with hccl](https://github.com/HabanaAI/Model-References/blob/1.6.0/PyTorch/computer_vision/classification/torchvision/utils.py#L249)\n",
    "```\n",
    "    from habana_frameworks.torch.distributed.hccl import initialize_distributed_hpu\n",
    "    args.world_size, args.rank, args.local_rank = initialize_distributed_hpu()\n",
    "    ...\n",
    "    dist.init_process_group(backend='hccl', rank=args.rank, world_size=args.world_size)\n",
    "```\n",
    "\n",
    "2. [Use the torch distributed data sampler](https://github.com/HabanaAI/Model-References/blob/1.6.0/PyTorch/computer_vision/classification/torchvision/train.py#L179)\n",
    "```\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
    "```\n",
    "\n",
    "3. [Distributed data parallel pytorch model initalization](https://github.com/HabanaAI/Model-References/blob/1.6.0/PyTorch/computer_vision/classification/torchvision/train.py#L328)\n",
    "```\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model, broadcast_buffers=False,\n",
    "            gradient_as_bucket_view=True)\n",
    "```\n",
    "\n",
    "__Note__: In Step 3, you must use the DistributedDataParallel API as DataParallel API is not supported by Habana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env PYTHONPATH=/home/ubuntu/Model-References/PyTorch/computer_vision/classification/torchvision:/root/examples/models:/usr/lib/habanalabs/:/root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ubuntu/Model-References/PyTorch/computer_vision/classification/torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the following patch to use CIFAR data and remove evaluation (you can pass -R to git apply if you want to revert it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git apply /home/ubuntu/DL1-Workshop/PyTorch-ResNet152/cifar_no_eval.patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following bash command as a shell script in the final cell(demo_resnet.sh) to start multi-HPU training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ```bash\n",
    "  export MASTER_ADDR=localhost\n",
    "  export MASTER_PORT=12355\n",
    "  /opt/amazon/openmpi/bin/mpirun -n 8 --bind-to core --map-by slot:PE=6 --rank-by core --report-bindings --allow-run-as-root \\\n",
    "    python3 train.py --model=resnet152 --device=hpu --batch-size=256 --epochs=90 --workers=10 \\\n",
    "    --dl-worker-type=MP --print-freq=10 --output-dir=. --seed=123 --hmp --hmp-bf16 ./ops_bf16_Resnet.txt \\\n",
    "    --hmp-fp32 ./ops_fp32_Resnet.txt --custom-lr-values 0.275 0.45 0.625 0.8 0.08 0.008 0.0008 \\\n",
    "    --custom-lr-milestones 1 2 3 4 30 60 80 --deterministic --dl-time-exclude=False\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sh /home/ubuntu/DL1-Workshop/PyTorch-ResNet152/demo_resnet.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUMMARY\n",
    "\n",
    "In this workshop, we did the following:\n",
    "- Downloaded and previewed the CIFAR dataset.\n",
    "- Learnt the steps needed for migrating the model to Habana HPU.\n",
    "- Learnt about HMP usage and why it is important.\n",
    "- Trained the Resnet152 model on single HPU.\n",
    "- Re-configured the training script for multi-node training and trained on 8 HPUs.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "92265e7bf95517031b05ae8ffa1541004d740e0704a96d3b488bf9f3a9b868ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
