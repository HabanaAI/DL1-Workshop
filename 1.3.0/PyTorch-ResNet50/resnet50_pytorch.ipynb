{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set PythonPath and cd into appropriate directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env PYTHONPATH=/home/ubuntu/work/Model-References/PyTorch/computer_vision/classification/torchvision:/root/examples/models:/usr/lib/habanalabs/:/root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ubuntu/work/Model-References/PyTorch/computer_vision/classification/torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries for pytorch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2021, Habana Labs Ltd.  All rights reserved.\n",
    "from __future__ import print_function\n",
    "\n",
    "#Import local copy of the model only for ResNext101_32x4d\n",
    "#which is not part of standard torchvision package.\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "import random\n",
    "import utils\n",
    "from resnet50_notebook_utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main training function\n",
    "\n",
    "Insert the following code block in the appropriate places (after backward loss computation and after optimizer step).\n",
    "\n",
    "```\n",
    "if args.run_lazy_mode:\n",
    "    import habana_frameworks.torch.core as htcore\n",
    "    htcore.mark_step()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, criterion, optimizer, data_loader, device, epoch, print_freq):\n",
    "    model.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \",device=device)\n",
    "    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value}'))\n",
    "    metric_logger.add_meter('img/s', utils.SmoothedValue(window_size=10, fmt='{value}'))\n",
    "\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    step_count = 0\n",
    "    last_print_time= time.time()\n",
    "\n",
    "    for image, target in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        image, target = image.to(device, non_blocking=True), target.to(device, non_blocking=True)\n",
    "\n",
    "        dl_ex_start_time=time.time()\n",
    "\n",
    "        if args.channels_last:\n",
    "            image = image.contiguous(memory_format=torch.channels_last)\n",
    "\n",
    "\n",
    "        if args.run_lazy_mode:\n",
    "            import habana_frameworks.torch.core as htcore\n",
    "            htcore.mark_step()\n",
    "\n",
    "        output = model(image)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # performance gain seen for these models using this mark_step.\n",
    "        if args.run_lazy_mode:\n",
    "            import habana_frameworks.torch.core as htcore\n",
    "            htcore.mark_step()\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        ##\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        ##\n",
    "\n",
    "        if step_count % print_freq == 0:\n",
    "            output_cpu = output.detach().to('cpu')\n",
    "            acc1, acc5 = utils.accuracy(output_cpu, target, topk=(1, 5))\n",
    "            batch_size = image.shape[0]\n",
    "            metric_logger.update(loss=loss.item(), lr=optimizer.param_groups[0][\"lr\"])\n",
    "            metric_logger.meters['acc1'].update(acc1.item(), n=batch_size*print_freq)\n",
    "            metric_logger.meters['acc5'].update(acc5.item(), n=batch_size*print_freq)\n",
    "            current_time = time.time()\n",
    "            last_print_time = dl_ex_start_time if args.dl_time_exclude else last_print_time\n",
    "            metric_logger.meters['img/s'].update(batch_size*print_freq / (current_time - last_print_time))\n",
    "            last_print_time = time.time()\n",
    "\n",
    "        step_count = step_count + 1\n",
    "        if step_count >= args.num_train_steps:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicate command line args for single HPU resnet50 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MAX_WAIT_ATTEMPTS\"] = \"50\"\n",
    "os.environ['HCL_CPU_AFFINITY'] = '1'\n",
    "os.environ['PT_HPU_ENABLE_SYNC_OUTPUT_HOST'] = 'false'\n",
    "parser = get_resnet50_argparser()\n",
    "   \n",
    "\n",
    "args = parser.parse_args([\"--batch-size\", \"256\", \"--epochs\", \"20\", \"--workers\", \"12\",\n",
    "\"--dl-time-exclude\", \"False\", \"--print-freq\", \"20\", \"--channels-last\", \"True\", \"--seed\", \"123\",\n",
    "\"--run-lazy-mode\", \"--hmp\",  \"--hmp-bf16\", \"/home/ubuntu/work/Model-References/PyTorch/computer_vision/classification/torchvision/ops_bf16_Resnet.txt\",\n",
    "\"--hmp-fp32\", \"/home/ubuntu/work/Model-References/PyTorch/computer_vision/classification/torchvision/ops_fp32_Resnet.txt\",\n",
    "\"--deterministic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main training loop for single node training. Use fake data to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from habana_frameworks.torch.utils.library_loader import load_habana_module\n",
    "load_habana_module()\n",
    "\n",
    "try:\n",
    "    # Default 'fork' doesn't work with synapse. Use 'forkserver' or 'spawn'\n",
    "    torch.multiprocessing.set_start_method('spawn')\n",
    "except RuntimeError:\n",
    "    pass\n",
    "\n",
    "if args.run_lazy_mode:\n",
    "    os.environ[\"PT_HPU_LAZY_MODE\"] = \"1\"\n",
    "if args.is_hmp:\n",
    "    from habana_frameworks.torch.hpex import hmp\n",
    "    hmp.convert(opt_level=args.hmp_opt_level, bf16_file_path=args.hmp_bf16,\n",
    "                fp32_file_path=args.hmp_fp32, isVerbose=args.hmp_verbose)\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if args.deterministic:\n",
    "    seed = args.seed\n",
    "    random.seed(seed)\n",
    "\n",
    "else:\n",
    "    seed = None\n",
    "\n",
    "device = torch.device('hpu')\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "dataset = datasets.FakeData(transform=transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,]))\n",
    "dataset_test = datasets.FakeData(transform=transforms.Compose([transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,]))\n",
    "\n",
    "\n",
    "train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "test_sampler = torch.utils.data.SequentialSampler(dataset_test)\t\n",
    "\n",
    "if args.workers > 0:\n",
    "    # patch torch cuda functions that are being unconditionally invoked\n",
    "    # in the multiprocessing data loader\n",
    "    torch.cuda.current_device = lambda: None\n",
    "    torch.cuda.set_device = lambda x: None\n",
    "\n",
    "data_loader_type = torch.utils.data.DataLoader\n",
    "\n",
    "data_loader = data_loader_type(\n",
    "    dataset, batch_size=args.batch_size, sampler=train_sampler,\n",
    "    num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "data_loader_test = data_loader_type(\n",
    "    dataset_test, batch_size=args.batch_size, sampler=test_sampler,\n",
    "    num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "print(\"Creating model\")\n",
    "model = torchvision.models.__dict__['resnet50'](pretrained=False)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if args.run_lazy_mode:\n",
    "    from habana_frameworks.torch.hpex.optimizers import FusedSGD\n",
    "    sgd_optimizer = FusedSGD\n",
    "else:\n",
    "    sgd_optimizer = torch.optim.SGD\n",
    "optimizer = sgd_optimizer(\n",
    "    model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n",
    "\n",
    "permute_params(model, True, args.run_lazy_mode)\n",
    "permute_momentum(optimizer, True, args.run_lazy_mode)\n",
    "\n",
    "model_for_train = model\n",
    "\n",
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    train_one_epoch(model_for_train, criterion, optimizer, data_loader,\n",
    "            device, epoch, print_freq=args.print_freq)\n",
    "\n",
    "\n",
    "if args.run_lazy_mode:\n",
    "    os.environ.pop(\"PT_HPU_LAZY_MODE\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Training\n",
    "\n",
    "**Restart the kernel before running the next section of the notebook**\n",
    "\n",
    "We will use the Model-References repo command line to demo multinode training. \n",
    "\n",
    "Multinode training differs in the following ways.\n",
    "\n",
    "1. Initialization with hccl\n",
    "```\n",
    "import habana_frameworks.torch.core.hccl    \n",
    "dist.init_process_group(backend='hccl', rank=rank, world_size=world_size)\n",
    "```\n",
    "2. Omit mark steps in lazy mode \n",
    "3. Use the torch distributed data sampler. ex:\n",
    "```\n",
    "        train_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
    "```\n",
    "4. Distributed data parallel pytorch model initalization. ex:\n",
    "```\n",
    "model = torch.nn.parallel.DistributedDataParallel(model, bucket_cap_mb=bucket_size_mb, broadcast_buffers=False,\n",
    "                    gradient_as_bucket_view=is_grad_view)\n",
    "```",
    "__Note__: regarding steps 3/4 you must use the DistributedDataParallel API, the DataParallel API is unsupported by Habana"
    
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env PYTHONPATH=/home/ubuntu/work/Model-References/PyTorch/computer_vision/classification/torchvision:/root/examples/models:/usr/lib/habanalabs/:/root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/ubuntu/work/Model-References/PyTorch/computer_vision/classification/torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the following patch to use fake data and remove evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git apply ~/fake_data_no_eval.patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -u demo_resnet.py  --world-size 8 --batch-size 256 --model resnet50 --device hpu --print-freq 1 \\\n",
    "  --channels-last True --deterministic --data-path $HOME --mode lazy \\\n",
    "  --epochs 30 --data-type bf16  --custom-lr-values 0.275,0.45,0.625,0.8,0.08,0.008,0.0008 \\\n",
    "  --custom-lr-milestones 1,2,3,4,30,60,80 --dl-time-exclude=False --dl-worker-type=MP"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e73c6261d5efc2f443ecd308a224101ada29c3472580d6a302997362509c15d1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
